<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>ENPH 353</title>
  <link rel="stylesheet" href="../styles.css">
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

  <nav class="navbar">
    <div class="nav-left">
      <a href="../index.html">Alfred Gunawan</a>
    </div>
    <div class="nav-right">
      <a href="../projects.html">Projects</a>
    </div>
  </nav>

  <header class="page-header">
    <div class="header-content">
      <a href="../projects.html" class="back-button">
        <i class="fas fa-arrow-left"></i> Back to Projects
      </a>
      <h1>ENPH 353: ML-Powered Robot Detective</h1>
      <p class="subtitle">Spring 2025</p>
    </div>
  </header>



  <div class="content-container">
    <!-- Main Content -->
    <div class="main-content">
      <section id ="Skills Practiced">
        <h2>Skills Practiced</h2>
        <ol>
            <li>Python</li>
            <li>Machine Learning and Neural Networks</li>
            <li>Data Analysis</li>
            <li>Algorithms</li>
        </ol>
      </section>
      <section id="Overview+Background">
        <h2>Overview+Background</h2>
        <p>
          ENPH 353 was a project course aimed at introducing students to machine learning techniques and
          getting us familiar with the concepts of data collection/analysis, model training, and working in a
          linux environment. The project of this course was a competition to create the best robot detective 
          that could independently drive around a predefined course and read clues from boards placed around the map. 
        </p>
        <figure class="description-image">
            <img src="../assets/enph353/mapOverview.png" alt="353 Map">
            <figcaption>Simulated Competition Map</figcaption>
        </figure>
        <figure class="description-image">
            <img src="../assets/enph353/clueboardExample.png" alt="353 Clueboard">
            <figcaption>An example of a clueboard</figcaption>
        </figure>
        <p>
          To give slightly more detail, the robot detective was a simulated self-driving car that had to navigate a course
          while avoiding obstacles and reading clueboards using computer vision. The clueboards contained alphanumeric characters
          that the robot had to read and report back to a central server. The robot was scored based on how many clueboards it
          could successfully read within a time limit as well as some other factors such as good driving and collisions.
        </p>
        <p>
          Before discussing this project further, I want to preface this by saying this page is not intended to be comprehensive at all.
          At the end of the course, our instructors required us to create a report with a much more complete overview of
          the technical details of the project. This report can be found <a href="../assets/enph353/ENPH_353_Final_Report.pdf" target="_blank" style="color: blue; text-decoration: underline;">here</a>. 
          Additionally, I also documented my experimentation in a google colab notebook which can be found <a href="https://colab.research.google.com/drive/1PeZeJDiXuhr8eD7yGDdbYNWeRfGkBjfO#scrollTo=AcwLL4iCwDIr" target="_blank" style="color: blue; text-decoration: underline;">here</a>.
          There're also some more resources useful for anyone wanting to take a deeper dive into my iterative process which can be found in This
          <a href="https://drive.google.com/drive/folders/1i6Xj6-i438DQ6IKZgsq-w6gbODPk8Vm9?usp=sharing" target="_blank" style="color: blue; text-decoration: underline;">google drive</a> that also contains the previous colab notebook.
        </p>  
        <p>  
          What I will be discussing on this page is more a brief overview of the concepts and skills I applied as well as a condensed summary of how our robot worked.
          I want to provide a page that is readable without ML knowledge and that still gives a clear insight into my experience with this project.
          Lastly, I would like to thank my teammate, Rachel, for her contributions to this project. We wouldn't have scored as much as we did 
          without her work.
        </p>
      </section>
      <section id="Agent Structure">
        <h2>Agent Structure</h2>
        <p>
          This section will be brief since this content is detailed in the report; the robot detective was structured as an agent 
          that contained two modules:
        </p>
        <ul>
            <li><strong>Driving Module:</strong> The module that controlled navigation around the course </li>
            <li><strong>Clue Reading Module:</strong> The module that controlled clue interpretation and submission </li>
        </ul>
        <p>
          Its physical body was essentially a cuboid on wheels with two cameras to its diagonal left and right.
          As mentioned in the report, each module was developed and tested independently before being integrated into the final agent.
          I was responsible for the clue reading module so that, and any concepts related to it, are what I'll be discussing on this page.
        </p>
      </section>
      <section id="Clue Reading">
        <h2>Clue Reading: Image Processing</h2>
        <p>
          The main problem involved in clue reading was how do we take an image of a clueboard and extract the alphanumeric characters from it?
          Well, this course was all about machine learning so I wanted to apply some ML techniques to solve this problem.
          In particular, one concept felt perfect for this problem: Convolutional Neural Networks (CNNs). I'll discuss CNNs more 
          in a later section and why they're ideal for this problem, but for now, we can view CNNs as functions that take an
          image as an input and spit out a character.
        </p>
        <p>
          Continuing with our analogy of CNNs as functions, CNNs have well-defined inputs and outputs. Since, I wanted the behaviour of
          the CNN to take in images from the agent's cameras and output what character they represent, I defined the input as a 108x108 pixel
          grayscale image and output as one of 36 classes: the 26 letters of the alphabet and the 10 digits. So with
          our "function" well-defined, I just had to get training data which fit this input-output structure. This is a whole problem in itself
          considering a camera image could look like this:
        </p>
        <figure class="description-image">
            <img src="../assets/enph353/cameraImageExample.png" alt="353 Camera Image">
            <figcaption>A frame from the agent's left camera</figcaption>
        </figure>
        <p>
          Clearly, this image contains a lot more information than just the clueboard characters. There's a lot more than just the clueboard 
          characters in this image and any CNN would have trouble distilling this down into just one character since there are multiple.
          We want to make the CNN's job as easy as possible and we can do this by isolating each character into its own image. This is 
          easily achievable with the open-source <a href="https://docs.opencv.org/4.x/index.html" target="_blank" style="color: blue; text-decoration: underline;">CV2 library</a>
          in Python - a library designed for image processing tasks. While seemingly daunting at first, 
          the problem of isolating characters can be broken down into a series of simple steps:
        </p>
        <ol>
          <li>Isolate the clueboard from the rest of the image</li>
          <li>Binarize the clueboard image (turn it into a black and white image)</li>
          <li>Find the largest contour to further isolate the white box where the text will be</li>
          <li>Use the contours in that region to extract characters</li>
        </ol>
        <p>
          There's a decent amount of technical detail involved in this process so if you're interested, check out the colab notebook I linked earlier.
          Anyways, after applying these steps, we can transform the original camera image into character images that look like this:
        </p>
        <figure class="description-image">
            <img src="../assets/enph353/tExample.png" alt="353 T">
            <figcaption>A "T" extracted from a clueboard</figcaption>
        </figure>
        <figure class="description-image">
            <img src="../assets/enph353/wExample.png" alt="353 W">
            <figcaption>A "W" extracted from a clueboard</figcaption>
        </figure>
        <p>
          Now that I had character images, I used these to train the CNN (details on this in the next section) so that it 
          could learn to recognize characters from images like these. Once that was complete, using the CNN is as simple
          as feeding in these character images and getting the predicted character as an output. Add in some logic for clue
          submission and our robot can score points!
        </p>  
      </section>
      <section id="Machine Learning and Neural Networks">
        <h2>Machine Learning and Neural Networks</h2>
        <p>
          I kinda breezed past a lot of the details about how the CNN worked in the previous section since that's what this
          section is for (there will be some math). To oversimplify, machine learning is a field of computer science that focuses on systems
          that can learn from data to make predictions or decisions without being explicitly programmed for specific tasks.
          Where we would program a classical system or algorithm with a set of rules to follow, machine learning systems can take
          training data and learn patterns from it to make predictions on new, unseen data. Neural
          networks are a subset of machine learning models that are loosely inspired by the neurons in a human brain.
          They consist of layers of interconnected nodes (or "neurons") that process and transmit information.
        </p>
        <p>
          To go over the CNN I made for this project, I'd like to start by first building up from the basic components of a neural network.
          As the name suggests, neural networks are made up of layers which each contain neurons. Neurons are the basic processing unit
          of a neural network and each are essentially a mathematical function that takes in inputs, applies weights and biases to them, 
          and produces an output. Mathematically, a single neuron computes a weighted sum of its inputs, adds a bias term, and then applies a nonlinear
          activation function. If we denote the inputs as \(x_i\), the weights as \(w_i\), and the bias as \(b\), a single neuron can be represented as:
        </p>
        <p>
          \( y = \sigma\left(\sum_i w_i x_i + b\right) \)
        </p>
        <p>
          Where \(\sigma(\cdot)\) is an activation function. Common activation functions include ReLU (Rectified Linear Unit),
          sigmoid, and tanh. The key idea here is nonlinearity: without activation functions, a neural network would collapse into
          a single linear transformation and would be incapable of learning complex patterns.
        </p>
        <p>  
          A standard fully connected (or "dense") neural network stacks many of layers of neurons together. During training, every neuron
          adjusts its weights and biases to minimize a loss function, which measures how far its predictions are from
          the correct answers. This optimization is typically done using gradient descent and backpropagation, where gradients
          of the loss with respect to each parameter are computed efficiently and used to update the model. I won't talk about
          gradient descent and backpropagation here but there are plenty of resources online that explain these concepts really well
          such as <a href="https://www.youtube.com/watch?v=VMj-3S1tku0" target="_blank" style="color: blue; text-decoration: underline;">this amazing video by Andrej Karpathy</a>.
        </p>
        <figure class="description-image">
            <img src="../assets/enph353/MLP.png" alt="MLP example">
            <figcaption>An example neural network, specifically a multilayer perceptron (MLP). Each dot is a neuron that
            applies weights, biases, and an activation function to its inputs to produce an output which gets fed into the next
            neuron.
            </figcaption>
        </figure>
        <p>
          While fully connected networks work well for some problems, they scale poorly for image data. Images have strong
          spatial structure: nearby pixels are highly correlated, and the same features (edges, corners, shapes) can appear
          anywhere in the image. Treating every pixel as an independent input ignores this structure and leads to an explosion
          in the number of parameters.
        </p>
        <p>
          This is where Convolutional Neural Networks (CNNs) come in. CNNs are specifically designed to process grid-like data,
          such as images. Instead of connecting every input pixel to every neuron, CNNs use convolutional layers that apply
          small learnable filters (or kernels) across the image. Each filter slides over the input and computes a dot product
          with local regions, producing a feature map that highlights where certain patterns appear.
        </p>
        <figure class="description-image">
            <img src="../assets/enph353/CNNKernels.webp" alt="Kernels example">
            <figcaption> How kernels operate on an image to produce feature maps.</figcaption>
        </figure>
        <p>
          Early convolutional layers tend to learn simple features like edges and gradients. As we move deeper into the network,
          later layers combine these low-level features into more abstract representations, such as shapes or even entire
          objects. This hierarchical feature learning is one of the main strengths of CNNs.
          CNNs often include pooling layers, which downsample feature maps by taking the maximum or average value over small
          regions. Pooling reduces the spatial resolution of the data, making the network more computationally efficient and
          more robust to small translations or noise in the input.
        </p>
        <figure class="description-image">
            <img src="../assets/enph353/CNNlayers.jpeg" alt="CNN example">
            <figcaption>Example layer structure of a CNN with convolutional, pooling, and fully connected layers. Note
              the reduction in spatial dimensions as we go deeper into the network.
            </figcaption>
        </figure>
        <p>
          In my model, the convolutional and pooling layers act as a feature extractor, transforming the raw image into a
          compact, high-level representation. This representation is then passed into one or more fully connected layers that
          perform the final classification. The network is trained end-to-end, meaning the filters, weights, and biases are
          all learned simultaneously from data. 
        </p>
        <p>
          That was a lot of info but to summarise, neural networks are powerful and flexible tools that we can utilise
          for complex tasks. CNNs, in particular, are well-suited for image data due to their ability to capture spatial hierarchies
          of features which is why I utilised them for character reading. There's so much interesting math behind machine learning
          but for this specific application, the core idea is that we can train a CNN to recognize characters from training data
          and use it to read clueboards for our robot detective.
        </p>
      </section>
      <section id="Conclusion">
        <h2>Conclusion</h2>
        <p>
          ENPH 353 was one of my favorite courses I've taken at UBC so far. It was such a fun and engaging way to
          get introduced to machine learning and being able to so directly apply the enthralling math behind it was so interesting.
          I find it very rare that courses teach you interesting and useful knowledge and skills while also allowing you to immediately apply
          them. I have a much bigger appreciation for and interest in machine learning and I would take this course again if I could.
        </p>
        <p>
          I find one of the most valuable things I've gotten from this course is the ability to work independently. A lot of the course
          was self-guided and while there were some amazing instructors and TAs to help, a lot of the learning came from my own experimentation.
          I've gotten a lot of confidence in my ability to learn new concepts and skills on my own and I definitely want to explore
          machine learning further and just more math in general.
        </p>
      </section>
    </div>

    <!-- Table of Contents -->
    <aside class="toc">
      <h3>Contents</h3>
      <ul>
        <li><a href="#Overview+Background">Overview+Background</a></li>
        <li><a href="#Agent Structure">Agent Structure</a></li>
        <li><a href="#Clue Reading">Clue Reading</a></li>
        <li><a href="#Machine Learning and Neural Networks">Machine Learning and Neural Networks</a></li>
        <li><a href="#Conclusion">Conclusion</a></li>
      </ul>
    </aside>
  </div>

</body>
</html>